{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_reduce.py\n",
    "# Defines a single function, map_reduce, which takes an input\n",
    "# dictionary i and applies the user-defined function mapper to each\n",
    "# (input_key,input_value) pair, producing a list of intermediate \n",
    "# keys and intermediate values.  Repeated intermediate keys then \n",
    "# have their values grouped into a list, and the user-defined \n",
    "# function reducer is applied to the intermediate key and list of \n",
    "# intermediate values.  The results are returned as a list.\n",
    "\n",
    "import itertools\n",
    "\n",
    "def map_reduce(i,mapper,reducer):\n",
    "    intermediate = []\n",
    "    for (key,value) in i.items():\n",
    "        intermediate.extend(mapper(key,value))\n",
    "    groups = {}\n",
    "    for key, group in itertools.groupby(sorted(intermediate), lambda x: x[0]): \n",
    "        groups[key] = list([y for x, y in group])\n",
    "    return [reducer(intermediate_key,groups[intermediate_key]) for intermediate_key in groups] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagerank_mr.py\n",
    "#\n",
    "# Computes PageRank, using a simple MapReduce library.\n",
    "#\n",
    "# MapReduce is used in two separate ways: (1) to compute\n",
    "# the inner product between the vector of dangling pages\n",
    "# (i.e., pages with no outbound links) and the current\n",
    "# estimated PageRank vector; and (2) to actually carry\n",
    "# out the update of the estimated PageRank vector.\n",
    "#\n",
    "# For a web of one million webpages the program consumes\n",
    "# about one gig of RAM, and takes an hour or so to run,\n",
    "# on a (slow) laptop with 3 gig of RAM, running Vista and\n",
    "# Python 2.5.\n",
    "\n",
    "from . import map_reduce\n",
    "import numpy.random\n",
    "import random\n",
    "\n",
    "def paretosample(n,power=2.0):\n",
    "    # Returns a sample from a truncated Pareto distribution\n",
    "    # with probability mass function p(l) proportional to\n",
    "    # 1/l^power.  The distribution is truncated at l = n.\n",
    "\n",
    "    m = n+1\n",
    "    while m > n: m = numpy.random.zipf(power)\n",
    "    return m\n",
    "\n",
    "def initialize(n,power):\n",
    "  # Returns a Python dictionary representing a web\n",
    "  # with n pages, and where each page k is linked to by\n",
    "  # L_k random other pages.  The L_k are independent and\n",
    "  # identically distributed random variables with a\n",
    "  # shifted and truncated Pareto probability mass function\n",
    "  # p(l) proportional to 1/(l+1)^power.\n",
    "\n",
    "  # The representation used is a Python dictionary with\n",
    "  # keys 0 through n-1 representing the different pages.\n",
    "  # i[j][0] is the estimated PageRank, initially set at 1/n,\n",
    "  # i[j][1] the number of outlinks, and i[j][2] a list of\n",
    "  # the outlinks.\n",
    "\n",
    "  # This dictionary is used to supply (key,value) pairs to\n",
    "  # both mapper tasks defined below.\n",
    "\n",
    "  # initialize the dictionary\n",
    "    i = {} \n",
    "    for j in range(n): i[j] = [1.0/n,0,[]]\n",
    "\n",
    "    # For each page, generate inlinks according to the Pareto\n",
    "    # distribution. Note that this is somewhat tedious, because\n",
    "    # the Pareto distribution governs inlinks, NOT outlinks,\n",
    "    # which is what our representation is adapted to represent.\n",
    "    # A smarter representation would give easy\n",
    "    # access to both, while remaining memory efficient.\n",
    "    for k in range(n):\n",
    "        lk = paretosample(n+1,power)-1\n",
    "        values = random.sample(range(n),lk)\n",
    "        for j in values:\n",
    "            i[j][1] += 1 # increment the outlink count for page j\n",
    "            i[j][2].append(k) # insert the link from j to k\n",
    "    return i\n",
    "\n",
    "def ip_mapper(input_key,input_value):\n",
    "  # The mapper used to compute the inner product between\n",
    "  # the vector of dangling pages and the current estimated\n",
    "  # PageRank.  The input is a key describing a webpage, and\n",
    "  # the corresponding data, including the estimated pagerank.\n",
    "  # The mapper returns [(1,pagerank)] if the page is dangling,\n",
    "  # and otherwise returns nothing.\n",
    "  \n",
    "    if input_value[1] == 0: return [(1,input_value[0])]\n",
    "    else: return []\n",
    "\n",
    "def ip_reducer(input_key,input_value_list):\n",
    "  # The reducer used to compute the inner product.  Simply\n",
    "  # sums the pageranks listed in the input value list, which\n",
    "  # are all the pageranks for dangling pages.\n",
    "\n",
    "    return sum(input_value_list)\n",
    "\n",
    "def pr_mapper(input_key,input_value):\n",
    "  # The mapper used to update the PageRank estimate.  Takes\n",
    "  # as input a key for a webpage, and as a value the corresponding\n",
    "  # data, as described in the function initialize.  It returns a\n",
    "  # list with all outlinked pages as keys, and corresponding values\n",
    "  # just the PageRank of the origin page, divided by the total\n",
    "  # number of outlinks from the origin page.  Also appended to\n",
    "  # that list is a pair with key the origin page, and value 0.\n",
    "  # This is done to ensure that every single page ends up with at\n",
    "  # least one corresponding (intermediate_key,intermediate_value)\n",
    "  # pair output from a mapper.\n",
    "  \n",
    "    return [(input_key,0.0)]+[(outlink,input_value[0]/input_value[1]) for outlink in input_value[2]]\n",
    "\n",
    "def pr_reducer_inter(intermediate_key,intermediate_value_list,\n",
    "                     s,ip,n):\n",
    "  # This is a helper function used to define the reducer used\n",
    "  # to update the PageRank estimate.  Note that the helper differs\n",
    "  # from a standard reducer in having some additional inputs:\n",
    "  # s (the PageRank parameter), ip (the value of the inner product\n",
    "  # between the dangling pages vector and the estimated PageRank),\n",
    "  # and n, the number of pages.  Other than that the code is\n",
    "  # self-explanatory.\n",
    "  \n",
    "    return (intermediate_key, s*sum(intermediate_value_list)+s*ip/n+(1.0-s)/n)\n",
    "\n",
    "def pagerank(i,s=0.85,tolerance=0.00001):\n",
    "  # Returns the PageRank vector for the web described by i,\n",
    "  # using parameter s.  The criterion for convergence is that\n",
    "  # we stop when M^(j+1)P-M^jP has length less than tolerance,\n",
    "  # in l1 norm.\n",
    "  \n",
    "    n = len(i)\n",
    "    iteration = 1\n",
    "    change = 2 # initial estimate of error\n",
    "    while change > tolerance:\n",
    "        print(\"Iteration: \" + str(iteration))\n",
    "        # Run the MapReduce job used to compute the inner product\n",
    "        # between the vector of dangling pages and the estimated\n",
    "        # PageRank.\n",
    "        ip_list = map_reduce(i,ip_mapper,ip_reducer)\n",
    "\n",
    "        # the if-else clause is needed in case there are no dangling\n",
    "        # pages, in which case MapReduce returns ip_list as the empty\n",
    "        # list.  Otherwise, set ip equal to the first (and only)\n",
    "        # member of the list returned by MapReduce.\n",
    "        if ip_list == []: ip = 0\n",
    "        else: ip = ip_list[0]\n",
    "\n",
    "        # Dynamically define the reducer used to update the PageRank\n",
    "        # vector, using the current values for s, ip, and n.\n",
    "        pr_reducer = lambda x,y: pr_reducer_inter(x,y,s,ip,n)\n",
    "\n",
    "        # Run the MapReduce job used to update the PageRank vector.\n",
    "        new_i = map_reduce(i,pr_mapper,pr_reducer)\n",
    "        \n",
    "        new_dict = {}\n",
    "        for x in new_i:\n",
    "            new_dict[x[0]] = x[1]\n",
    "       \n",
    "        # Compute the new estimate of error.\n",
    "        change = sum([abs(new_dict[j]-i[j][0]) for j in i])\n",
    "        print(\"Change in l1 norm: \" + str(change))\n",
    "        \n",
    "\n",
    "        # Update the estimate PageRank vector.\n",
    "        for j in i: \n",
    "            i[j][0] = new_dict[j]\n",
    "        iteration += 1\n",
    "    return i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PageRank-test.txt') as input:\n",
    "    lines = input.readlines()\n",
    "\n",
    "node_list = {}\n",
    "for line in lines:\n",
    "    _id, adj_list = line.strip().split('\\t', 1)\n",
    "    cmd = 'adj_list = %s' %adj_list\n",
    "    exec(cmd)\n",
    "    \n",
    "    \n",
    "    node = [1/len(lines), len(adj_list), list(adj_list.keys())]\n",
    "    node_list[_id] = node\n",
    "\n",
    "# print(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank(node_list,0.85,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagerank(i,0.85,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

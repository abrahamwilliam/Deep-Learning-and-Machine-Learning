{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Due June/20/2018 11:59:59 PM, PST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Review (10%)\n",
    "In a certain day care class, 30% of the children have grey eyes, 50% of them have blue and the other 20%'s eyes are in other colors. One day they play a game together. In the first run, 65% of the grey eye ones, 82% of the blue eyed ones and 50% of the children with other eye color were selected. Now, if a child is selected randomly from the class, and we know that he/she was not in the first game, what is the probability that the child has blue eyes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Review (10%)\n",
    "(1) What is Gram-Schmidt procedure ? \n",
    "<br>\n",
    "(2) Do Gram-Schmidt procedure for the vectors x1 = (1, 1 ,0), x2 = (1, 0, 1), x3 = (0, 1, 1) in $R^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian for Logistic Regression (10%)\n",
    "Consider the average empirical loss  for logistic regression:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\log (1 + e^{-y_i \\theta^T x_i}) = - \\frac{1}{m} \\sum_{i=1}^{m} \\log (h_{\\theta}(y_i x_i)))$$\n",
    "\n",
    "where $h_{\\theta}(x) = g(\\theta^{T} x)$ and $g(z) = \\frac{1}{1 + e^{-z}}$. \n",
    "\n",
    "(1) Find the Hessian $H$ of $J(\\theta)$.\n",
    "<br>\n",
    "(2) Show that Hessian is non-negative definite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression model using stochastic gradient (40%)\n",
    "\n",
    "In this problem, we will train a linear regression model using stochastic gradient descent on the\n",
    "Wine Quality dataset. The data provide to train is \"winequality-white.csv\". We will partition the problem into following three parts:\n",
    "\n",
    "<br>\n",
    "\n",
    "A. Making Predictions\n",
    "\n",
    "<br>\n",
    "\n",
    "B. Esimating Coefficients\n",
    "\n",
    "<br>\n",
    "\n",
    "C. Wine Quality Testing\n",
    "\n",
    "The Wine Quality Dataset involves predicting the quality of white wines on a scale given\n",
    "chemical measures of each wine. It is a multiclass classi\f",
    "cation problem, but could also be\n",
    "framed as a regression problem. The number of observations for each class is not balanced.\n",
    "There are 4,898 observations with 11 input variables and 1 output variable. The variable names\n",
    "are as follows:\n",
    "\n",
    "1. Fixed acidity.\n",
    "2. Volatile acidity.\n",
    "3. Citric acid.\n",
    "4. Residual sugar.\n",
    "5. Chlorides.\n",
    "6. Free sulfur dioxide.\n",
    "7. Total sulfur dioxide.\n",
    "8. Density.\n",
    "9. pH.\n",
    "10. Sulphates.\n",
    "11. Alcohol.\n",
    "12. Quality (score between 0 and 10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible Python lib required to run this problem. \n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Making Prediction\n",
    "### Given dataset with 5 point, the first is input value, \n",
    "### and the second is the expected value\n",
    "\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6,6]]\n",
    "\n",
    "### Also given linear model coefficient as \n",
    "\n",
    "coef = [0.5, 0.8]\n",
    "\n",
    "### where the first component is b0 and the second component is b1 and \n",
    "### model is assumed as y = b0 + b1*x\n",
    "\n",
    "### Can you write a predict function to return predicted value?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Esimating Coefficients\n",
    "\n",
    "\n",
    "\n",
    "### Given dataset with 6 point, the first is input value, and the second is the expected value\n",
    "\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5], [6, 6]]\n",
    "\n",
    "l_rate = 0.01\n",
    "\n",
    "n_epoch = 50\n",
    "\n",
    "### Can you write a function to return coefficients based on stochastic gradient descent method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### C. Wine Quality Testing\n",
    "\n",
    "If we partition the data set uniformly into 6 parts, could you write a code to get the average value of absoulte error between prediction and expected (average over these 6 partitoned datasets)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier for Digit Recognition (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (70000, 784)\n",
      "label shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the digit data either from mldata.org, or once downloaded to data_home, from disk. The data is about 53MB so this cell\n",
    "# should take a while the first time your run it.\n",
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "\n",
    "# Rescale grayscale values to [0,1].\n",
    "X = X / 255.0\n",
    "\n",
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "print('data shape: ', X.shape)\n",
    "print('label shape:', Y.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "test_data, test_labels = X[61000:], Y[61000:]\n",
    "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
    "train_data, train_labels = X[:60000], Y[:60000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create a 10x10 grid to visualize 10 examples of each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  Fit a Naive Bayes classifier and report accuracy on the dev data. Remember that Naive Bayes estimates P(feature|label). While sklearn can handle real-valued features, let's start by mapping the pixel values to either 0 or 1. You can do this as a preprocessing step, or with the binarize argument. With binary-valued features, you can use BernoulliNB. Next try mapping the pixel values to 0, 1, or 2, representing white, grey, or black. This mapping requires MultinomialNB. Does the multi-class version improve the results? Why or why not?\n",
    "### Note, the answers may vary depending on what thresholds you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Try training a model using GuassianNB, which is intended for real-valued features, and evaluate on the dev data. You'll notice that it doesn't work so well. Could you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

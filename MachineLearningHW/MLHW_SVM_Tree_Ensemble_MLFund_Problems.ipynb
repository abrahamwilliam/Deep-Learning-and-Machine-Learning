{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Due July/4/2018 11:59:59 PM, PST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel (10%)\n",
    "\n",
    "Could you prove or disprove by counter example about following constructed functions (left hand side) are kernels or not? All right hand side functions $K_$ are assumed kernels already. Notaions: $x, z$ are $n$-dim vectors. $a$ is a positive real number. $f : \\mathbb{R}^{n} -> \\mathbb{R}$ is a real-valued function. $\\phi : \\mathbb{R}^{n} -> \\mathbb{R}^{n}$ is a $n$-dim to $n$-dim function.  \n",
    "\n",
    "(a) $K(x; z) = K_1(x; z) + K_2(x; z)$\n",
    "<br>\n",
    "(b) $K(x; z) = K_1(x; z) - K_2(x; z)$\n",
    "<br>\n",
    "(c) $K(x; z) = aK_1(x; z)$\n",
    "<br>\n",
    "(d) $K(x; z) = -aK_1(x; z)$\n",
    "<br>\n",
    "(e) $K(x; z) = K_1(x; z)K_2(x; z)$\n",
    "<br>\n",
    "(f) $K(x; z) = f(x)f(z)$\n",
    "<br>\n",
    "(g) $K(x; z) = K_3(\\phi(x); \\phi(z))$\n",
    "<br>\n",
    "(h) $K(x; z) = p(K_1(x; z))$, where $p$ is a polynomial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VC Dimension (10%)\n",
    "\n",
    "(a) Let two hypothesis classes $H_1$ and $H_2$ satisfy $H_1 \\in H_2$. Prove or disprove: $VC(H_1) \\leq VC(H_2)$.\n",
    "\n",
    "(b) Let $H_1 = H_2 \\cup H_3$. Prove or disprove: $VC(H_1) \\leq  VC(H_2) + VC(H_3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Decision Tree (30%)\n",
    "\n",
    "In this assignment, we try to build decision tree from scratch. We will separate this task into following subtasks.\n",
    "1. Gini Index\n",
    "\n",
    "2. Create Split based on Gini Index\n",
    "\n",
    "3. Build Tree\n",
    "\n",
    "4. Make a Prediction\n",
    "\n",
    "5. Case study by Decision Tree Built\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gini Index\n",
    "\n",
    "A Gini score gives an idea of how good a split is by measuring how mixed the classes are in the two groups created by the split. A gini index for classifying two classes (attributes) into two groups can be represented as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "gini_{index} &= propor\\_class\\_0\\_at\\_gp\\_1 \\times (1 - propor\\_class\\_0\\_at\\_gp\\_1)) \\\\\n",
    "             &+ propor\\_class\\_1\\_at\\_gp\\_1 \\times (1 - propor\\_class\\_1\\_at\\_gp\\_1)) \\\\\n",
    "             &+ propor\\_class\\_0\\_at\\_gp\\_2 \\times (1 - propor\\_class\\_0\\_at\\_gp\\_2)) \\\\\n",
    "             &+ propor\\_class\\_1\\_at\\_gp\\_2 \\times (1 - propor\\_class\\_1\\_at\\_gp\\_2))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can try use groups of datasets and class values as input, and return gini index based.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Split based on Gini Index\n",
    "\n",
    "In this part, you will try to split a dataset and evaluate all splits to select best feature and its best splitting point. Best means to have smallest gini index.\n",
    "\n",
    "Given following data with 10 rows and each row contains two features X1 and X2, label by variable Y (0 or 1). Could you write a code to fine the best feature and best splitting points? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [X1, X2, Y] \n",
    "dataset = [[2.771244718,1.784783929,0],\n",
    "\t[1.728571309,1.169761413,0],\n",
    "\t[3.678319846,2.81281357,0],\n",
    "\t[3.961043357,2.61995032,0],\n",
    "\t[2.999208922,2.209014212,0],\n",
    "\t[7.497545867,3.162953546,1],\n",
    "\t[9.00220326,3.339047188,1],\n",
    "\t[7.444542326,0.476683375,1],\n",
    "\t[10.12493903,3.234550982,1],\n",
    "\t[6.642287351,3.319983761,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Tree\n",
    "Building a tree may be divided into 3 main parts:\n",
    "A. Terminal Nodes. (Can use two criteriosn, (1) maximum tree depth or (2) use minimum rows required to be contained by a node)\n",
    "B. Recursive Splitting.\n",
    "C. Building a Tree.\n",
    "\n",
    "For same dataset as above, could you show a tree with depth 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [X1, X2, Y] \n",
    "dataset = [[2.771244718,1.784783929,0],\n",
    "\t[1.728571309,1.169761413,0],\n",
    "\t[3.678319846,2.81281357,0],\n",
    "\t[3.961043357,2.61995032,0],\n",
    "\t[2.999208922,2.209014212,0],\n",
    "\t[7.497545867,3.162953546,1],\n",
    "\t[9.00220326,3.339047188,1],\n",
    "\t[7.444542326,0.476683375,1],\n",
    "\t[10.12493903,3.234550982,1],\n",
    "\t[6.642287351,3.319983761,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make a Prediction\n",
    "Given following test dataset, could you use the just built tree to make prediction?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [[1.771244718,1.784783929,0],\n",
    "           [2.728571309,1.169761413,0],\n",
    "[2.678319846,2.81281357,0],\n",
    "[2.961043357,2.61995032,0],\n",
    "[2.999208922,2.209014212,0],\n",
    "[0.497545867,3.162953546,0],\n",
    "[8.00220326,3.339047188,1],\n",
    "[8.444542326,0.476683375,1],\n",
    "[10.12493903,3.234550982,1],\n",
    "[7.642287351,3.319983761,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Case study by Decision Tree Built\n",
    "\n",
    "Using the provided dataset about banknote authentication (data_banknote_authentication.csv) file\n",
    "with following attributes. Please using developed decision tree to make prediction and evaluate your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier (20%)\n",
    "### Given iris data set (at sklearn datasets), could you compare classification results by plotting decision boundary for classifiers  linear, RBF (set kernel coefficient 5), and Polynomial (set degree 2) ? You can utilze sklearn svm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Aggregation (30%)\n",
    "\n",
    "Decision trees are a simple and powerful predictive modeling technique, but they often have high-variance problem. In this exercise, we will implement a bootstrap aggregation algorithm for decision trees. We will implment sample with replacement and use this to build a serieous of tree models for prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Resample (With Replacement)\n",
    "\n",
    "Can you write a function to implement resampling with replacement? \n",
    "For example, could you show different sample mean for different sample sizes for \n",
    "following data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building aggregated tree models.\n",
    "\n",
    "Could you use samples with replacement to bagging predict the data (sonar.all-data.csv)?\n",
    "Try to compare accuracy results for different bags size. For example, compare\n",
    "average prediction results for bags with tree numbers say, 1, 5, 10, 50.\n",
    "\n",
    "Data attributes, more details can be found at https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
